{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 08: Phrase Classification\n",
    "The assignment this week needs you to distinguish between good and bad phrases of the word \"**earn**\" (e.g., earn money). The method, word2vector, learned today will be used in the process. \n",
    "\n",
    "There're some data for this assignment: \n",
    "* train.tsv: Some phrases with labels to train and validate the classification model. There are only two types of label: 1 means *good*; 0 means *bad*.\n",
    "* test.tsv: Same format as train.tsv. It's used to test your model.\n",
    "* GoogleNews-vectors-negative300.bin.gz: a pre-trained word2vector model trained by Google ([source](https://code.google.com/archive/p/word2vec/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement\n",
    "* pandas\n",
    "* tensorflow\n",
    "* sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "We use dataframe to store data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         phrase  class\n",
      "0      earn a strong reputation      1\n",
      "1  Marty will surely earn every      0\n",
      "2             to earn between $      0\n",
      "3          to earn some college      0\n",
      "4        that earn rave reviews      0\n",
      "                   phrase  class\n",
      "0  degree earn 62 percent      0\n",
      "1     earn maybe 30 or 50      0\n",
      "2  earn the kind of money      1\n",
      "3      earn his 14th save      1\n",
      "4   earn a smaller amount      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def loadData(path):\n",
    "    ngram = []\n",
    "    _class = []\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip(\"\\n\").split(\"\\t\")\n",
    "            ngram.append(line[0])\n",
    "            _class.append(int(line[1]))\n",
    "    return pd.DataFrame({\"phrase\":ngram,\"class\":_class})\n",
    "train = loadData(\"train.tsv\")\n",
    "print(train.head())\n",
    "test = loadData(\"test.tsv\")    \n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load word2vec model\n",
    "<font color=\"red\">**[ TODO ]**</font> Please load [GoogleNews-vectors-negative300.bin.gz](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g) model and check the embedding of the word `language`.\n",
    "\n",
    "* package `gensim` is a good choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.30712891e-02,  1.68457031e-02,  1.54296875e-01,  1.27929688e-01,\n",
       "       -2.67578125e-01,  3.51562500e-02,  1.19140625e-01,  2.48046875e-01,\n",
       "        1.93359375e-01, -7.95898438e-02,  1.46484375e-01, -1.43554688e-01,\n",
       "       -3.04687500e-01,  3.46679688e-02, -1.85546875e-02,  1.06933594e-01,\n",
       "       -1.52343750e-01,  2.89062500e-01,  2.35595703e-02, -3.80859375e-01,\n",
       "        1.09863281e-01,  4.41406250e-01,  3.75976562e-02, -1.22680664e-02,\n",
       "        1.62353516e-02, -2.24609375e-01,  7.61718750e-02, -3.12500000e-02,\n",
       "       -2.16064453e-02,  1.49414062e-01, -4.02832031e-02, -4.46777344e-02,\n",
       "       -1.72851562e-01,  3.32031250e-02,  1.50390625e-01, -5.05371094e-02,\n",
       "        2.72216797e-02,  3.00781250e-01, -1.33789062e-01, -7.56835938e-02,\n",
       "        1.93359375e-01, -1.98242188e-01, -1.27563477e-02,  4.19921875e-01,\n",
       "       -2.19726562e-01,  1.44531250e-01, -3.93066406e-02,  1.94335938e-01,\n",
       "       -3.12500000e-01,  1.84570312e-01,  1.48773193e-04, -1.67968750e-01,\n",
       "       -7.37304688e-02, -3.12500000e-02,  1.57226562e-01,  3.30078125e-01,\n",
       "       -1.42578125e-01, -3.16406250e-01, -7.32421875e-02, -5.76171875e-02,\n",
       "        1.02050781e-01, -1.08886719e-01,  1.24023438e-01, -2.50244141e-02,\n",
       "       -2.49023438e-01,  1.25976562e-01, -1.79687500e-01,  3.32031250e-01,\n",
       "        7.14111328e-03,  2.51953125e-01,  4.34570312e-02, -4.34570312e-02,\n",
       "       -3.90625000e-01,  1.76757812e-01, -1.13525391e-02, -1.97753906e-02,\n",
       "        2.79296875e-01,  2.36328125e-01,  1.19140625e-01,  5.59082031e-02,\n",
       "        1.73828125e-01, -1.10839844e-01, -4.95605469e-02,  2.13867188e-01,\n",
       "        6.17675781e-02,  1.38671875e-01, -4.45556641e-03,  2.55859375e-01,\n",
       "        1.80664062e-01,  5.88378906e-02, -6.59179688e-02, -2.08007812e-01,\n",
       "       -1.19140625e-01, -1.57226562e-01,  5.02929688e-02, -6.29882812e-02,\n",
       "        5.00488281e-02, -7.27539062e-02,  1.74560547e-02, -3.56445312e-02,\n",
       "       -1.93359375e-01,  3.93066406e-02, -3.36914062e-02, -1.07421875e-01,\n",
       "        5.78613281e-02, -8.20312500e-02,  1.74560547e-02, -1.65039062e-01,\n",
       "        1.46484375e-01, -3.08837891e-02, -3.86718750e-01,  2.49023438e-01,\n",
       "        8.74023438e-02, -2.15820312e-01, -4.10156250e-02,  1.60156250e-01,\n",
       "        1.85546875e-01, -2.27050781e-02, -3.73535156e-02,  7.86132812e-02,\n",
       "       -1.46484375e-01,  6.78710938e-02,  1.26953125e-01,  3.30078125e-01,\n",
       "        1.11328125e-01,  9.27734375e-02, -3.45703125e-01, -1.41601562e-01,\n",
       "       -5.29785156e-02, -1.50390625e-01, -7.81250000e-02, -1.27929688e-01,\n",
       "       -4.02343750e-01, -1.41601562e-01,  8.44726562e-02,  1.08398438e-01,\n",
       "       -4.44335938e-02,  3.73535156e-02,  5.61523438e-02, -1.91406250e-01,\n",
       "        1.54296875e-01, -5.12695312e-02, -6.49414062e-02, -8.30078125e-02,\n",
       "        7.17773438e-02, -1.33789062e-01,  1.05468750e-01,  3.33984375e-01,\n",
       "       -1.08398438e-01,  1.91650391e-02,  2.14843750e-01,  2.15820312e-01,\n",
       "       -1.05468750e-01, -1.44531250e-01,  4.32128906e-02, -2.71484375e-01,\n",
       "       -3.78906250e-01,  1.09863281e-01, -8.15429688e-02, -6.12792969e-02,\n",
       "       -1.33789062e-01,  9.71679688e-02, -1.04370117e-02, -1.21093750e-01,\n",
       "       -2.44140625e-01,  1.02050781e-01,  1.10839844e-01, -1.00585938e-01,\n",
       "        1.71875000e-01, -3.61328125e-02, -4.39453125e-02,  2.83203125e-01,\n",
       "       -8.93554688e-02, -1.70898438e-01,  2.46093750e-01,  1.16699219e-01,\n",
       "        8.39843750e-02, -1.32812500e-01, -1.61132812e-01, -1.39648438e-01,\n",
       "       -8.59375000e-02, -1.37695312e-01, -9.32617188e-02, -1.33789062e-01,\n",
       "        1.65039062e-01,  4.93164062e-02, -1.21093750e-01, -2.11914062e-01,\n",
       "        1.61132812e-01, -1.07421875e-01, -3.97949219e-02, -3.51562500e-01,\n",
       "       -5.02929688e-02,  1.46484375e-01, -4.68750000e-02,  4.17480469e-02,\n",
       "       -1.27929688e-01, -9.76562500e-02, -2.46093750e-01,  6.78710938e-02,\n",
       "       -2.30468750e-01,  1.80664062e-02,  3.54003906e-02,  7.32421875e-02,\n",
       "       -2.23632812e-01, -1.25976562e-01,  2.12890625e-01, -3.93066406e-02,\n",
       "       -2.41699219e-02, -9.61914062e-02,  7.51953125e-02, -1.46484375e-01,\n",
       "       -1.49414062e-01, -8.83789062e-02, -4.88281250e-02,  2.32421875e-01,\n",
       "        3.30078125e-01,  1.59179688e-01, -2.35351562e-01, -1.25976562e-01,\n",
       "        2.68554688e-02, -5.29785156e-02, -6.59179688e-02, -2.17773438e-01,\n",
       "       -6.37817383e-03, -2.53906250e-01,  2.28515625e-01,  4.93164062e-02,\n",
       "        3.54003906e-02,  1.66992188e-01, -7.27539062e-02, -2.53906250e-01,\n",
       "       -1.34765625e-01,  3.69140625e-01,  1.83593750e-01, -1.64062500e-01,\n",
       "        2.26562500e-01, -8.88671875e-02,  3.69140625e-01,  5.54199219e-02,\n",
       "       -3.63769531e-02, -1.48437500e-01,  9.13085938e-02,  2.47955322e-04,\n",
       "        2.67578125e-01, -1.63085938e-01,  1.19628906e-01,  2.77343750e-01,\n",
       "       -1.49414062e-01,  1.33789062e-01, -8.25195312e-02, -1.74804688e-01,\n",
       "       -1.77734375e-01,  2.06054688e-01,  5.07812500e-02, -2.08007812e-01,\n",
       "       -1.74804688e-01,  9.66796875e-02,  6.98242188e-02, -5.79833984e-04,\n",
       "        9.22851562e-02,  7.95898438e-02,  1.41601562e-01,  8.72802734e-03,\n",
       "       -8.05664062e-02,  4.80957031e-02,  2.49023438e-01, -1.64062500e-01,\n",
       "       -4.66308594e-02, -2.81250000e-01, -1.66015625e-01, -2.22656250e-01,\n",
       "       -2.32421875e-01,  1.32812500e-01,  4.15039062e-02,  1.15234375e-01,\n",
       "       -7.66601562e-02, -1.10839844e-01, -1.97265625e-01,  3.06396484e-02,\n",
       "       -1.03515625e-01,  2.49023438e-02, -2.52685547e-02,  3.39355469e-02,\n",
       "        4.29687500e-02, -1.44531250e-01,  2.12402344e-02,  2.28271484e-02,\n",
       "       -1.88476562e-01,  3.22265625e-01, -1.13281250e-01, -7.61718750e-02,\n",
       "        2.94921875e-01, -1.33789062e-01, -1.80664062e-02, -6.25610352e-03,\n",
       "       -1.62353516e-02,  5.98144531e-02,  1.21582031e-01,  4.17480469e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import models\n",
    "\n",
    "w = models.KeyedVectors.load_word2vec_format(\n",
    "    'GoogleNews-vectors-negative300.bin', binary=True)\n",
    "#### print \"language\" embedding \n",
    "w['language']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Expected output: </font>\n",
    "\n",
    ">  <font face='monospace' size=3>\\[&nbsp;2.30712891e-02&nbsp;&nbsp;1.68457031e-02&nbsp;&nbsp;1.54296875e-01&nbsp; 1.27929688e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.67578125e-01&nbsp;&nbsp;3.51562500e-02&nbsp;&nbsp;1.19140625e-01&nbsp; 2.48046875e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.93359375e-01&nbsp;-7.95898438e-02&nbsp;&nbsp;1.46484375e-01&nbsp;-1.43554688e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-3.04687500e-01&nbsp;&nbsp;3.46679688e-02&nbsp;-1.85546875e-02&nbsp; 1.06933594e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.52343750e-01&nbsp;&nbsp;2.89062500e-01&nbsp;&nbsp;2.35595703e-02&nbsp;-3.80859375e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.09863281e-01&nbsp;&nbsp;4.41406250e-01&nbsp;&nbsp;3.75976562e-02&nbsp;-1.22680664e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.62353516e-02&nbsp;-2.24609375e-01&nbsp;&nbsp;7.61718750e-02&nbsp;-3.12500000e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.16064453e-02&nbsp;&nbsp;1.49414062e-01&nbsp;-4.02832031e-02&nbsp;-4.46777344e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.72851562e-01&nbsp;&nbsp;3.32031250e-02&nbsp;&nbsp;1.50390625e-01&nbsp;-5.05371094e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;2.72216797e-02&nbsp;&nbsp;3.00781250e-01&nbsp;-1.33789062e-01&nbsp;-7.56835938e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.93359375e-01&nbsp;-1.98242188e-01&nbsp;-1.27563477e-02&nbsp; 4.19921875e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.19726562e-01&nbsp;&nbsp;1.44531250e-01&nbsp;-3.93066406e-02&nbsp; 1.94335938e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-3.12500000e-01&nbsp;&nbsp;1.84570312e-01&nbsp;&nbsp;1.48773193e-04&nbsp;-1.67968750e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-7.37304688e-02&nbsp;-3.12500000e-02&nbsp;&nbsp;1.57226562e-01&nbsp; 3.30078125e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.42578125e-01&nbsp;-3.16406250e-01&nbsp;-7.32421875e-02&nbsp;-5.76171875e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.02050781e-01&nbsp;-1.08886719e-01&nbsp;&nbsp;1.24023438e-01&nbsp;-2.50244141e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.49023438e-01&nbsp;&nbsp;1.25976562e-01&nbsp;-1.79687500e-01&nbsp; 3.32031250e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;7.14111328e-03&nbsp;&nbsp;2.51953125e-01&nbsp;&nbsp;4.34570312e-02&nbsp;-4.34570312e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-3.90625000e-01&nbsp;&nbsp;1.76757812e-01&nbsp;-1.13525391e-02&nbsp;-1.97753906e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;2.79296875e-01&nbsp;&nbsp;2.36328125e-01&nbsp;&nbsp;1.19140625e-01&nbsp; 5.59082031e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.73828125e-01&nbsp;-1.10839844e-01&nbsp;-4.95605469e-02&nbsp; 2.13867188e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;6.17675781e-02&nbsp;&nbsp;1.38671875e-01&nbsp;-4.45556641e-03&nbsp; 2.55859375e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.80664062e-01&nbsp;&nbsp;5.88378906e-02&nbsp;-6.59179688e-02&nbsp;-2.08007812e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.19140625e-01&nbsp;-1.57226562e-01&nbsp;&nbsp;5.02929688e-02&nbsp;-6.29882812e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;5.00488281e-02&nbsp;-7.27539062e-02&nbsp;&nbsp;1.74560547e-02&nbsp;-3.56445312e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.93359375e-01&nbsp;&nbsp;3.93066406e-02&nbsp;-3.36914062e-02&nbsp;-1.07421875e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;5.78613281e-02&nbsp;-8.20312500e-02&nbsp;&nbsp;1.74560547e-02&nbsp;-1.65039062e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.46484375e-01&nbsp;-3.08837891e-02&nbsp;-3.86718750e-01&nbsp; 2.49023438e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;8.74023438e-02&nbsp;-2.15820312e-01&nbsp;-4.10156250e-02&nbsp; 1.60156250e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.85546875e-01&nbsp;-2.27050781e-02&nbsp;-3.73535156e-02&nbsp; 7.86132812e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.46484375e-01&nbsp;&nbsp;6.78710938e-02&nbsp;&nbsp;1.26953125e-01&nbsp; 3.30078125e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.11328125e-01&nbsp;&nbsp;9.27734375e-02&nbsp;-3.45703125e-01&nbsp;-1.41601562e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-5.29785156e-02&nbsp;-1.50390625e-01&nbsp;-7.81250000e-02&nbsp;-1.27929688e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-4.02343750e-01&nbsp;-1.41601562e-01&nbsp;&nbsp;8.44726562e-02&nbsp; 1.08398438e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-4.44335938e-02&nbsp;&nbsp;3.73535156e-02&nbsp;&nbsp;5.61523438e-02&nbsp;-1.91406250e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.54296875e-01&nbsp;-5.12695312e-02&nbsp;-6.49414062e-02&nbsp;-8.30078125e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;7.17773438e-02&nbsp;-1.33789062e-01&nbsp;&nbsp;1.05468750e-01&nbsp; 3.33984375e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.08398438e-01&nbsp;&nbsp;1.91650391e-02&nbsp;&nbsp;2.14843750e-01&nbsp; 2.15820312e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.05468750e-01&nbsp;-1.44531250e-01&nbsp;&nbsp;4.32128906e-02&nbsp;-2.71484375e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-3.78906250e-01&nbsp;&nbsp;1.09863281e-01&nbsp;-8.15429688e-02&nbsp;-6.12792969e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.33789062e-01&nbsp;&nbsp;9.71679688e-02&nbsp;-1.04370117e-02&nbsp;-1.21093750e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.44140625e-01&nbsp;&nbsp;1.02050781e-01&nbsp;&nbsp;1.10839844e-01&nbsp;-1.00585938e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.71875000e-01&nbsp;-3.61328125e-02&nbsp;-4.39453125e-02&nbsp; 2.83203125e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-8.93554688e-02&nbsp;-1.70898438e-01&nbsp;&nbsp;2.46093750e-01&nbsp; 1.16699219e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;8.39843750e-02&nbsp;-1.32812500e-01&nbsp;-1.61132812e-01&nbsp;-1.39648438e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-8.59375000e-02&nbsp;-1.37695312e-01&nbsp;-9.32617188e-02&nbsp;-1.33789062e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.65039062e-01&nbsp;&nbsp;4.93164062e-02&nbsp;-1.21093750e-01&nbsp;-2.11914062e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.61132812e-01&nbsp;-1.07421875e-01&nbsp;-3.97949219e-02&nbsp;-3.51562500e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-5.02929688e-02&nbsp;&nbsp;1.46484375e-01&nbsp;-4.68750000e-02&nbsp; 4.17480469e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.27929688e-01&nbsp;-9.76562500e-02&nbsp;-2.46093750e-01&nbsp; 6.78710938e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.30468750e-01&nbsp;&nbsp;1.80664062e-02&nbsp;&nbsp;3.54003906e-02&nbsp; 7.32421875e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.23632812e-01&nbsp;-1.25976562e-01&nbsp;&nbsp;2.12890625e-01&nbsp;-3.93066406e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.41699219e-02&nbsp;-9.61914062e-02&nbsp;&nbsp;7.51953125e-02&nbsp;-1.46484375e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.49414062e-01&nbsp;-8.83789062e-02&nbsp;-4.88281250e-02&nbsp; 2.32421875e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;3.30078125e-01&nbsp;&nbsp;1.59179688e-01&nbsp;-2.35351562e-01&nbsp;-1.25976562e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;2.68554688e-02&nbsp;-5.29785156e-02&nbsp;-6.59179688e-02&nbsp;-2.17773438e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-6.37817383e-03&nbsp;-2.53906250e-01&nbsp;&nbsp;2.28515625e-01&nbsp; 4.93164062e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;3.54003906e-02&nbsp;&nbsp;1.66992188e-01&nbsp;-7.27539062e-02&nbsp;-2.53906250e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.34765625e-01&nbsp;&nbsp;3.69140625e-01&nbsp;&nbsp;1.83593750e-01&nbsp;-1.64062500e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;2.26562500e-01&nbsp;-8.88671875e-02&nbsp;&nbsp;3.69140625e-01&nbsp; 5.54199219e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-3.63769531e-02&nbsp;-1.48437500e-01&nbsp;&nbsp;9.13085938e-02&nbsp; 2.47955322e-04<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;2.67578125e-01&nbsp;-1.63085938e-01&nbsp;&nbsp;1.19628906e-01&nbsp; 2.77343750e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.49414062e-01&nbsp;&nbsp;1.33789062e-01&nbsp;-8.25195312e-02&nbsp;-1.74804688e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.77734375e-01&nbsp;&nbsp;2.06054688e-01&nbsp;&nbsp;5.07812500e-02&nbsp;-2.08007812e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.74804688e-01&nbsp;&nbsp;9.66796875e-02&nbsp;&nbsp;6.98242188e-02&nbsp;-5.79833984e-04<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;9.22851562e-02&nbsp;&nbsp;7.95898438e-02&nbsp;&nbsp;1.41601562e-01&nbsp; 8.72802734e-03<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-8.05664062e-02&nbsp;&nbsp;4.80957031e-02&nbsp;&nbsp;2.49023438e-01&nbsp;-1.64062500e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-4.66308594e-02&nbsp;-2.81250000e-01&nbsp;-1.66015625e-01&nbsp;-2.22656250e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.32421875e-01&nbsp;&nbsp;1.32812500e-01&nbsp;&nbsp;4.15039062e-02&nbsp; 1.15234375e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-7.66601562e-02&nbsp;-1.10839844e-01&nbsp;-1.97265625e-01&nbsp; 3.06396484e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.03515625e-01&nbsp;&nbsp;2.49023438e-02&nbsp;-2.52685547e-02&nbsp; 3.39355469e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;4.29687500e-02&nbsp;-1.44531250e-01&nbsp;&nbsp;2.12402344e-02&nbsp; 2.28271484e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.88476562e-01&nbsp;&nbsp;3.22265625e-01&nbsp;-1.13281250e-01&nbsp;-7.61718750e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;2.94921875e-01&nbsp;-1.33789062e-01&nbsp;-1.80664062e-02&nbsp;-6.25610352e-03<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.62353516e-02&nbsp;&nbsp;5.98144531e-02&nbsp;&nbsp;1.21582031e-01&nbsp; 4.17480469e-02\\] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Preprocess two tsv files here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adjust the ratio of the two classes of training data\n",
    "In training data, the ratio of good phrases to bad phrases is about one to thirty. That will make training classification unsatisfactory, so we need to adjust the ratio. Reducing bad phrases and adding good phrases are both common way.\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Please adjust the ratio of good phrases to bad phrases in any way which you think is the best and output the number of two class for demo.\n",
    "\n",
    "You need to explain why you choose this ratio and how you do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以增加好片語的資料量，來提升比例。\n",
    "\n",
    "或減少壞片語的量。\n",
    "\n",
    "輸出兩類的筆數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrase    6105\n",
      "class     6105\n",
      "dtype: int64\n",
      "phrase    6105\n",
      "class     6105\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "total_train = train[train['class']==1].append(train[train['class']==0].sample(6105, random_state=1))\n",
    "total_train = shuffle(total_train)\n",
    "train = total_train\n",
    "\n",
    "print(train[train['class']==1].count())\n",
    "print(train[train['class']==0].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number words\n",
    "Let each word have its unique number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(pd.concat([train,test],ignore_index=True)['phrase'])\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "\n",
    "#tok.word_index: 為 dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert phrases into numbers\n",
    "Because model can't read words, so we have to do this transform. \n",
    "\n",
    "The number should be same as the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded_phrase = tok.texts_to_sequences(train['phrase'])\n",
    "test_encoded_phrase = tok.texts_to_sequences(test['phrase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### padding\n",
    "Make all phrases become same length. The longest phrases in two tsv have five tokens. Hence, we should make the phrases whose lengths less than five become five by adding 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  14    1 1167  189 1680]\n",
      " [   1    5   96    4    0]\n",
      " [   1   27   36    0    0]\n",
      " [   1  102  136  775   17]\n",
      " [   7  126    2    1  378]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_ngram = 5\n",
    "X_train= pad_sequences(train_encoded_phrase, maxlen=max_ngram, padding='post')\n",
    "X_test= pad_sequences(test_encoded_phrase, maxlen=max_ngram, padding='post')\n",
    "print(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31395     0\n",
       "17702     0\n",
       "70089     1\n",
       "197406    0\n",
       "124297    0\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['class'][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one hot encodding label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train=to_categorical(train['class'])\n",
    "y_test=to_categorical(test['class'])\n",
    "print(y_train[:5])\n",
    "\n",
    "# 前面一項代表 '0', 後面一項代表 '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split training data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating the embedding matrix\n",
    "The embedding matrix is used by classification model. It should be a list of list. Each sub-list is an embedding vector of a word and the order of all embedding vectors should be same as *tokenizer*. It is stored in a dictionary. You can check it by `tok.word_index()`.\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Make embedding matrix. If you don't need it for your classification model, you can skip it. We won't check it when demo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Keras—embedding嵌入層的用法詳解](https://www.796t.com/article.php?id=22887) 包含 embedding matrix 作法\n",
    "\n",
    "**The embedding matrix will always have the number of columns equal to the number of the embedding dimension and the row count will be equal to the number of unique words in the document or a user-defined number of rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dim = 300\n",
    "embedding_matrix = np.zeros((vocab_size, dim))\n",
    "\n",
    "for word, i in tok.word_index.items():\n",
    "    try:\n",
    "        embedding_vec = w[word]\n",
    "        if embedding_vec is not None:\n",
    "            embedding_matrix[i] = embedding_vec\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build model\n",
    "<font color=\"red\">**[ TODO ]**</font> Please build your classification model by ***keras*** here. \n",
    "\n",
    "You **must** use the pre-trained word2vec model to represent the words of phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推薦使用 LSTM\n",
    "\n",
    "- [Day 05：Keras 模型、函數及參數使用說明](https://ithelp.ithome.com.tw/articles/10191725)\n",
    "\n",
    "How to embedding initialize with `embedding matrix`? \n",
    "\n",
    "- [How to use a pre-trained embedding matrix in tensorflow 2.0 RNN as initial weights in an embedding layer?](https://stackoverflow.com/questions/55770009/how-to-use-a-pre-trained-embedding-matrix-in-tensorflow-2-0-rnn-as-initial-weigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "model = tf.keras.models.Sequential([\n",
    "    # Embedding layer 只能加在第一層, 且透過 `embedding_initializer` 來利用 embedding_matrix 來初始化\n",
    "    tf.keras.layers.Embedding(vocab_size, dim, input_length = 5, embeddings_initializer=keras.initializers.constant(embedding_matrix)),\n",
    "\n",
    "    tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.52),  \n",
    "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.41),  \n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 5, 300)            979500    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 5, 128)            219648    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 5, 64)             49408     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 320)               1280      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               82176     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 1,332,526\n",
      "Trainable params: 1,331,886\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train\n",
    "Train classification model here.\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Adjust the hyperparameter to optimize the validation accuracy and validation loss.\n",
    "\n",
    "* The higher the accuracy, the better; the lower the validation, the better.\n",
    "* **number of epoch** and **batch size** are the most important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10/10 [==============================] - 5s 217ms/step - loss: 0.6084 - accuracy: 0.6732 - val_loss: 0.6643 - val_accuracy: 0.7584\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 1s 146ms/step - loss: 0.3983 - accuracy: 0.8257 - val_loss: 0.6130 - val_accuracy: 0.7445\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 2s 151ms/step - loss: 0.2999 - accuracy: 0.8738 - val_loss: 0.5892 - val_accuracy: 0.7019\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 1s 148ms/step - loss: 0.2399 - accuracy: 0.9092 - val_loss: 0.5623 - val_accuracy: 0.7207\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 1s 147ms/step - loss: 0.1882 - accuracy: 0.9326 - val_loss: 0.5261 - val_accuracy: 0.7625\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.1512 - accuracy: 0.9475 - val_loss: 0.5038 - val_accuracy: 0.7432\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 2s 161ms/step - loss: 0.1220 - accuracy: 0.9587 - val_loss: 0.4723 - val_accuracy: 0.7678\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 2s 181ms/step - loss: 0.1053 - accuracy: 0.9675 - val_loss: 0.4726 - val_accuracy: 0.7011\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 2s 176ms/step - loss: 0.0861 - accuracy: 0.9736 - val_loss: 0.4612 - val_accuracy: 0.6978\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 2s 156ms/step - loss: 0.0725 - accuracy: 0.9784 - val_loss: 0.4276 - val_accuracy: 0.7428\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 1s 147ms/step - loss: 0.0643 - accuracy: 0.9827 - val_loss: 0.3972 - val_accuracy: 0.7789\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 2s 151ms/step - loss: 0.0549 - accuracy: 0.9841 - val_loss: 0.3931 - val_accuracy: 0.7715\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 1s 150ms/step - loss: 0.0503 - accuracy: 0.9863 - val_loss: 0.3381 - val_accuracy: 0.8673\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 1s 143ms/step - loss: 0.0442 - accuracy: 0.9876 - val_loss: 0.2843 - val_accuracy: 0.9255\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 1s 143ms/step - loss: 0.0426 - accuracy: 0.9875 - val_loss: 0.3072 - val_accuracy: 0.8857\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 1s 148ms/step - loss: 0.0351 - accuracy: 0.9897 - val_loss: 0.2603 - val_accuracy: 0.9218\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 1s 147ms/step - loss: 0.0385 - accuracy: 0.9889 - val_loss: 0.2213 - val_accuracy: 0.9578\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 0.0372 - accuracy: 0.9898 - val_loss: 0.1995 - val_accuracy: 0.9726\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 1s 149ms/step - loss: 0.0326 - accuracy: 0.9895 - val_loss: 0.1618 - val_accuracy: 0.9791\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 1s 151ms/step - loss: 0.0353 - accuracy: 0.9902 - val_loss: 0.1744 - val_accuracy: 0.9713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb3f835f4a8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train,y_train,validation_data=(X_val,y_val), epochs=20, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Test your model by test.tsv and output the accuracy. Your accuracy need to beat baseline: **0.97**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1778 - accuracy: 0.9730\n",
      "0.9729999899864197\n"
     ]
    }
   ],
   "source": [
    "# Keras中 model.evaluate（）返回的是 损失值和你选定的指标值（例如，精度accuracy）\n",
    "accuracy = model.evaluate(X_test,y_test)\n",
    "print(accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show wrong prediction results\n",
    "Observing wrong prediction result may help you improve your prediction.\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> show the wrong prediction results like this: \n",
    "\n",
    "<img src=\"https://imgur.com/BOTMyZH.jpg\" width=30%><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Python | Get key from value in Dictionary](https://www.geeksforgeeks.org/python-get-key-from-value-in-dictionary/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = list(tok.word_index.keys())\n",
    "value_list = list(tok.word_index.values())\n",
    "result_ngram = []   # X_test 的 ngram, 也就是原來的sentence\n",
    "\n",
    "# Return text from word vector\n",
    "for i in range(len(X_test)):\n",
    "    result_sublist = []\n",
    "    X_test_sublist = list(X_test[i])\n",
    "    for j in range(len(X_test_sublist)):\n",
    "        if X_test_sublist[j] == 0:\n",
    "            break\n",
    "        else:\n",
    "            position = value_list.index(X_test_sublist[j])\n",
    "            result_sublist.append(key_list[position])\n",
    "    \n",
    "    result_ngram.append(\" \".join(result_sublist))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Python numpy.argmax()](https://www.delftstack.com/zh-tw/api/numpy/python-numpy-argmax/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每一列為預測為 0 or 1 的機率，因此我們取較高者為最後的預測結果\n",
    "\n",
    "y_predict = np.argmax(model.predict(X_test), axis=1)\n",
    "y_test_1= np.argmax(y_test, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "\n",
    "result['ngram'] = result_ngram\n",
    "result['label'] = y_test_1\n",
    "result['predict'] = y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>label</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>degree earn 62 percent</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>earn maybe 30 or 50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>earn the kind of money</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>earn his 14th save</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>earn a smaller amount</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>earn many times that</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>earn such money</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>earn free gear with bonus</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>way to earn profits</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>right you 'll earn</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          ngram  label  predict\n",
       "0        degree earn 62 percent      0        0\n",
       "1           earn maybe 30 or 50      0        0\n",
       "2        earn the kind of money      1        1\n",
       "3            earn his 14th save      1        1\n",
       "4         earn a smaller amount      1        1\n",
       "...                         ...    ...      ...\n",
       "1995       earn many times that      1        1\n",
       "1996            earn such money      1        1\n",
       "1997  earn free gear with bonus      1        1\n",
       "1998        way to earn profits      0        0\n",
       "1999         right you 'll earn      1        1\n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>label</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>right to try to earn</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>earn your masters degree</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>earn ones living</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>earn its 70 interest</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>earn and keep it</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>earn a living</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>earn big profit</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>earn your commission</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>earn real money</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>earn some pocket money</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>earn a living</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>earn affiliate income</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>more you earn</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>earn their money</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>earn pension entitlements enabling them</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>earn a 3 0 victory</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>earn the gold award</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>earn you a living</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>more we earn</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>earn income</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>what they can earn</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>earn on average</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>earn copyright protection</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>earn or get money</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>earn money earn money</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>what i earn</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>earn immediately money</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>earn money online earn money</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>earn spot</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>earn an independent income</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363</th>\n",
       "      <td>earn a share of that</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>earn their profit</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>earn an income</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>earn another income</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>more productive earn</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>earn and to save</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>earn earned income</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>him to earn</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>earn free gear with bonus</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>million people who earn less</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>earn money</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>earn money earn money</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>earn yet more money</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>them try to earn</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>earn their credit</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>earn extra money genuine</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>earn to qualify for what</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>earn more</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>earn profit</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>earn the most</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>money earn</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912</th>\n",
       "      <td>earn fake money</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>earn the same profit</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>clear advantage and earn more</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ngram  label  predict\n",
       "157                      right to try to earn      1        0\n",
       "197                  earn your masters degree      0        1\n",
       "264                          earn ones living      1        0\n",
       "301                      earn its 70 interest      1        0\n",
       "422                          earn and keep it      1        0\n",
       "519                             earn a living      1        0\n",
       "567                           earn big profit      1        0\n",
       "641                      earn your commission      0        1\n",
       "643                           earn real money      1        0\n",
       "696                    earn some pocket money      1        0\n",
       "724                             earn a living      1        0\n",
       "741                     earn affiliate income      0        1\n",
       "770                             more you earn      0        1\n",
       "831                          earn their money      1        0\n",
       "867   earn pension entitlements enabling them      1        0\n",
       "897                        earn a 3 0 victory      1        0\n",
       "948                       earn the gold award      0        1\n",
       "949                         earn you a living      1        0\n",
       "986                              more we earn      1        0\n",
       "1021                              earn income      1        0\n",
       "1054                       what they can earn      0        1\n",
       "1081                          earn on average      0        1\n",
       "1135                earn copyright protection      0        1\n",
       "1175                        earn or get money      1        0\n",
       "1201                    earn money earn money      1        0\n",
       "1245                              what i earn      0        1\n",
       "1248                   earn immediately money      1        0\n",
       "1287             earn money online earn money      1        0\n",
       "1339                                earn spot      1        0\n",
       "1353               earn an independent income      1        0\n",
       "1363                     earn a share of that      1        0\n",
       "1420                        earn their profit      1        0\n",
       "1427                           earn an income      1        0\n",
       "1449                      earn another income      1        0\n",
       "1462                     more productive earn      1        0\n",
       "1463                         earn and to save      1        0\n",
       "1470                       earn earned income      1        0\n",
       "1501                              him to earn      1        0\n",
       "1512                earn free gear with bonus      0        1\n",
       "1526             million people who earn less      0        1\n",
       "1536                               earn money      1        0\n",
       "1585                    earn money earn money      1        0\n",
       "1588                      earn yet more money      1        0\n",
       "1612                         them try to earn      1        0\n",
       "1622                        earn their credit      1        0\n",
       "1629                 earn extra money genuine      0        1\n",
       "1696                 earn to qualify for what      1        0\n",
       "1719                                earn more      1        0\n",
       "1721                              earn profit      1        0\n",
       "1795                            earn the most      1        0\n",
       "1808                               money earn      0        1\n",
       "1912                          earn fake money      1        0\n",
       "1917                     earn the same profit      1        0\n",
       "1951            clear advantage and earn more      0        1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result['label'] != result['predict']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TA's Notes\n",
    "\n",
    "If you complete the Assignment, please use [this link](https://docs.google.com/spreadsheets/d/1QGeYl5dsD9sFO9SYg4DIKk-xr-yGjRDOOLKZqCLDv2E/edit#gid=807282025) to reserve demo time.  \n",
    "The score is only given after TAs review your implementation, so <u>**make sure you make a appointment with a TA before you miss the deadline**</u> .  <br>After demo, please upload your assignment to eeclass. You just need to hand in this ipynb file and rename it as XXXXXXXXX(Your student ID).ipynb.\n",
    "<br>Note that **late submission will not be allowed**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Resource\n",
    "[Deep Learning with Python](https://tanthiamhuat.files.wordpress.com/2018/03/deeplearningwithpython.pdf)\n",
    "\n",
    "[Classification on IMDB](https://keras.io/examples/nlp/bidirectional_lstm_imdb/)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07d37431c3a3df9bcf522a81701087d631d582cf2af9e5eb1fee4b0875132d49"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('nlp': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
