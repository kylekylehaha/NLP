{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Week 14: Senquence Classification with BERT\n",
    "\n",
    "The assignment this week is to do the senquence classification. This may sound like what we had done in the previous assignment, but we are using BERT as our classifier this week, rather than Machine Learning.\n",
    "\n",
    "The objective is to judge the CEFR level of a sentence.  \n",
    "[CEFR](https://www.cambridgeenglish.org/exams-and-tests/cefr/) is a standard for describing language ability of a person. It consists of 6 levels, A1, A2, B1, B2, C1, and C2, going from easier to harder.  \n",
    "A dataset that contains sentences with the corresponding CEFR level is provided, and you have to use BERT and train a sentence classifier with this dataset.  \n",
    "The dataset is collected and processed from a research by Alison Chi, 李書卉, 李冠霖 and Prof. Chang. Thank you all for allowing us to use it in the lecture.\n",
    "\n",
    "As to the implementatin, we will introduce you the [🤗 transformers](https://huggingface.co/) library, which is mantained by huggingface company, as the training framework this week. [Pytorch](https://pytorch.org/) is used as the deep learning backend in this tutorial, but with the transformers library, all codes can be easily changed to tensorflow if you prefer so.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare your environment\n",
    "\n",
    "Again, we highly recommend you to install all packages with a virtual environment manager, like [venv](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/) or [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html), to prevent version conflicts of different packages.  \n",
    "\n",
    "If you haven't used it before and don't know which to use, I would suggest you start with [mamba](https://github.com/mamba-org/mamba#installation) or [mambaforge](https://github.com/conda-forge/miniforge#mambaforge)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Install CUDA\n",
    "\n",
    "Deep learning is a computionally extensive process. It takes lots of time if relying only on the CPU, especially when it's trained on a large dataset. That's why using GPU instead is generally recommended.  \n",
    "To use GPU for computation, you have to install [CUDA toolkit](https://developer.nvidia.com/cuda-toolkit) as well as the [cuDNN library](https://developer.nvidia.com/cudnn) provided by NVIDIA.  \n",
    "\n",
    "If you already had CUDA installed on your machine, then great! You're done here.  \n",
    "If you don't, you can refer to [Appendix 1](#Appendix-1-Install-CUDA) to see how to do so."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Install python packages\n",
    "\n",
    "Dependencies:\n",
    "\n",
    "1. `numpy`: for matrix operation\n",
    "2. `scikit-learn`: for label encoding\n",
    "3. `datasets`: for data preparation\n",
    "4. `transformers`: for model loading and finetuing\n",
    "5. (choose one) `tensorflow` / `pytorch`: the backend DL framework\n",
    "   - Note that the tf/pt version must support the CUDA version you've installed if you want to use GPU.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Select GPU(s) for your backend\n",
    "\n",
    "Skip this section if you have no intension of using GPU with tensorflow/pytorch."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "\r\n",
    "# select your GPU. Note that this should be set before you load tensorflow or pytorch.\r\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\r\n",
    "\r\n",
    "# To use multiple GPUs, combine all GPU ID with commas\r\n",
    "# e.g. >>> os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,3'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### >> Check Pytorch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch\r\n",
    "# Check if any GPU is used\r\n",
    "torch.cuda.is_available()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# the model you want to use. Available models can be found here: https://huggingface.co/models\r\n",
    "MODEL_NAME = 'distilbert-base-uncased'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data\n",
    "\n",
    "Similar to `transformers` library, `datasets` is also a package provided by huggingface. It contains many public datasets online and can help us with the data processing.  \n",
    "We can use `load_dataset` function to read the input `.csv` file.\n",
    "\n",
    "Reference:\n",
    " - [Official datasets document](https://huggingface.co/docs/datasets)\n",
    " - [datasets.load_dataset](https://huggingface.co/docs/datasets/loading.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import os\r\n",
    "from datasets import load_dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "dataset = load_dataset('csv', data_files = os.path.join('data', 'evp.train.csv'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-ea3d0f52fdc6a6f9\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading and preparing dataset csv/default to /Users/yenlingkai/.cache/huggingface/datasets/csv/default-ea3d0f52fdc6a6f9/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2519.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 328.19it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset csv downloaded and prepared to /Users/yenlingkai/.cache/huggingface/datasets/csv/default-ea3d0f52fdc6a6f9/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 160.85it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing\n",
    "\n",
    "Same as before, texts should be tokenized, embedded, and padded before put into the model.  \n",
    "But don't worry, with the libraries from huggingface, the procedure is much easier now."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Sentence processing\n",
    "\n",
    "Different pre-trained language models may have their own preprocessing models, and that's why we should use the tokenizers trained along with that model. In our case, we are using distilBERT, so we should use the distilBERT tokenizer.  \n",
    "\n",
    "With huggingface, loading different tokenizer is extremely easy: just import the AutoTokenizer from `transformers` and tell it what model you plan to use, and it will handle everything for you.\n",
    "\n",
    "Reference:\n",
    " - [transformers.AutoTokenizer](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from transformers import AutoTokenizer # For tokenization\r\n",
    "\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Play with BERTTokenizer\n",
    "\n",
    "<small><i>*You can safely skip this section if you're already familar with BERTTokenizer.</i></small>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's play with this tokenizer a little bit before we go on.\n",
    "\n",
    "Using this tokenizer is pretty easy: just call this object, and it processes the sentences for you.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "example = \"This so-called \\\"Perfect Evening\\\" was so disappointing, as well as discouraging us from coming to your Circle Theatre again.\"\r\n",
    "\r\n",
    "embeddings = tokenizer(example)\r\n",
    "embeddings"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2061, 1011, 2170, 1000, 3819, 3944, 1000, 2001, 2061, 15640, 1010, 2004, 2092, 2004, 12532, 4648, 4726, 2149, 2013, 2746, 2000, 2115, 4418, 3004, 2153, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, the sentence has already been tokenized and embedded. A default attention mask is returned as well.  \n",
    "\n",
    "To get the token back is easy as well!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "decoded_tokens = tokenizer.batch_decode(embeddings['input_ids'])\r\n",
    "print(' '.join(decoded_tokens))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[CLS] this so - called \" perfect evening \" was so disappointing , as well as disco ##ura ##ging us from coming to your circle theatre again . [SEP]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You may notice that there're some weird stuffs appearing in your task, like `[CLS]` or `[SEP]`. The word *discouraging* is even split into `disco` `##ura` and `##ging` .  \n",
    "`[CLS]`, `[SEP]`, `[UKN]` and `[MASK]` are four symbols introduced by BERT, which stand for \"classification\", \"seperator\", \"unknown\" and \"mask\" respectively.  \n",
    "As to `##` thing, it's called a *wordpiece*, which is a concept [also brought out by Google](https://arxiv.org/abs/1609.08144). The key idea is to split words into common sub-word units, so the number of rare words can significantly decrease.\n",
    "\n",
    "Besides simply tokenizing a sentence, there are also many parameters you can set. You can play with it a bit, changing the parameters and observe the difference.\n",
    "\n",
    "Document:\n",
    " - [transformers.Tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# EXAMPLE: directly transform into embedding tensor\r\n",
    "embeddings = tokenizer(example,\r\n",
    "                       # padding='longest',         # padding strategy\r\n",
    "                       # max_length=10,             # how long to pad sentences\r\n",
    "                       is_split_into_words=False,\r\n",
    "                       truncation=True,\r\n",
    "                       return_tensors='pt',         # 'tf' for tensofrlow, 'pt' for pytorch, 'np' for numpy\r\n",
    "                       # return_length=True         # whether to return length\r\n",
    "                       # Any other parameters you want to try\r\n",
    "                      )\r\n",
    "embeddings"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2023,  2061,  1011,  2170,  1000,  3819,  3944,  1000,  2001,\n",
       "          2061, 15640,  1010,  2004,  2092,  2004, 12532,  4648,  4726,  2149,\n",
       "          2013,  2746,  2000,  2115,  4418,  3004,  2153,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Label processing\n",
    "\n",
    "Before we start to process sentences in the whole dataset, don't forget we need to process labels as well.\n",
    "\n",
    "In the following section, I will introduce you the OneHotEncoder provided by scikit-learn.\n",
    "\n",
    "Documents:\n",
    " - [sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import numpy as np\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "\r\n",
    "# First, declare a new encoder\r\n",
    "encoder = OneHotEncoder(sparse = False)\r\n",
    "# Then, let the encoder learns all features in the given dataset\r\n",
    "# Keep in mind that all `fit` functions in sklearn only make the encoder learn from the data, not transforming the data yet.\r\n",
    "encoder = encoder.fit(np.reshape(dataset['train']['level'], (-1, 1)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "LABEL_COUNT = len(encoder.categories_[0])\r\n",
    "print(LABEL_COUNT)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Play with OneHotEncoder\n",
    "\n",
    "<small><i>*You can safely skip this section if you're already familar with sklearn.</i></small>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One thing you should always keep in mind is: features learned by OneHotEncoder are always treated as arrays, because it allows multi-field features. (See its [document](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder) for an example)  \n",
    "That's why you have to reshape the level into (-1, 1), i.e. from `['A1', 'B1', 'C1', ...]` to `[['A1'], ['B1'], ['C1'], ...]` ."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Let's see what features has the encoder captured\r\n",
    "print(encoder.categories_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[array(['A1', 'A2', 'B1', 'B2', 'C1', 'C2'], dtype='<U2')]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "# use `encoder.transform` to get the one-hot code of a label\r\n",
    "print(encoder.transform([['B1']]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "encoder.transform([[dataset['train'][0]['level']]])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# To decode, use `encoder.inverse_transform` instead\r\n",
    "print(encoder.inverse_transform([[0, 0, 1, 0, 0, 0]]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['B1']]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "encoder.transform([[dataset['train'][1]['level']]])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### [ TODO ] Process the data\n",
    "\n",
    "With the tokenizor and encoder prepared, we can write a function to process our dataset!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def preprocess(dataslice):\r\n",
    "    \"\"\" Input: a batch of your dataset\r\n",
    "        Example: { 'text': [['sentence1'], ['setence2'], ...],\r\n",
    "                   'label': ['label1', 'label2', ...] }\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    # [ TODO ]\r\n",
    "    embedding = tokenizer(dataslice['text'])\r\n",
    "    dataslice['input_ids'] = embedding['input_ids']\r\n",
    "    dataslice['attention_mask'] = embedding['attention_mask']\r\n",
    "    dataslice['label'] = encoder.transform(np.reshape(dataslice['level'], (-1, 1)))\r\n",
    "\r\n",
    "    \"\"\" Output: a batch of processed dataset\r\n",
    "        Example: { 'input_ids': ...,\r\n",
    "                   'attention_masks': ...,\r\n",
    "                   'label': ... }\r\n",
    "    \"\"\"\r\n",
    "    return dataslice\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, map the function to the whole dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "processed_data = dataset.map(preprocess,    # your processing function\r\n",
    "                             batched = True # Process in batches so it can be faster\r\n",
    "                            )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 21/21 [00:01<00:00, 16.15ba/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# Take a look at processed dataset\r\n",
    "print(processed_data)\r\n",
    "processed_data['train'][0]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['attention_mask', 'input_ids', 'label', 'level', 'text'],\n",
      "        num_rows: 20720\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'input_ids': [101, 2026, 2388, 2003, 2383, 2014, 2482, 13671, 1012, 102],\n",
       " 'label': [0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'level': 'B1',\n",
       " 'text': 'My mother is having her car repaired.'}"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparation\n",
    "\n",
    "We can load the pretrained model from `transformers`.  \n",
    "Generally, you need to build your own model on top of BERT if you want to use BERT for some downstream tasks, but again, sequence classification is a popular topic. With the support from `transformers` library, all works can be done in two lines of codes: \n",
    "\n",
    "1. Load `AutoModelForSequenceClassification` Class.\n",
    "2. Load the pretrained model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "# Change to TFAutoModelForSequenceClassification if you're using tensoflow\r\n",
    "from transformers import AutoModelForSequenceClassification\r\n",
    "\r\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME,\r\n",
    "                                                           num_labels = LABEL_COUNT)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 256M/256M [01:26<00:00, 3.09MB/s] \n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### [ TODO ] Split train/val data\n",
    "\n",
    "The `Dataset` class we prepared before already has the `train_test_split` method. You can use it to split your dataset.\n",
    "\n",
    "Document:\n",
    " - [datasets.Dataset - Sort, shuffle, select, split, and shard](https://huggingface.co/docs/datasets/process.html#sort-shuffle-select-split-and-shard)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "# [ TODO ] Choose the validation data size                                v here\r\n",
    "train_val_dataset = processed_data['train'].train_test_split(test_size = 0.2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "# Take a look at split data\r\n",
    "print(train_val_dataset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['attention_mask', 'input_ids', 'label', 'level', 'text'],\n",
      "        num_rows: 16576\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['attention_mask', 'input_ids', 'label', 'level', 'text'],\n",
      "        num_rows: 4144\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### [ TODO ] Setup training parameters\n",
    "\n",
    "We are using the TrainerAPI to do the training. Trainer is yet another utility provided by huggingface, which helps you train the model with ease.  \n",
    "\n",
    "Document:\n",
    "- [transformers.TrainingArguments](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments)\n",
    "- [transformers.Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "# Change to TFTrainingArguments, TFTrainer if you're using tensoflow\r\n",
    "from transformers import TrainingArguments, Trainer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "# [ TODO ] Set and tune your training properties\r\n",
    "LEARNING_RATE = 0.3\r\n",
    "# BATCH_SIZE = ...\r\n",
    "EPOCH = 10\r\n",
    "training_args = TrainingArguments(\r\n",
    "    output_dir = 'model',\r\n",
    "    # learning_rate = LEARNING_RATE,\r\n",
    "    # per_device_train_batch_size = BATCH_SIZE,\r\n",
    "    # per_device_eval_batch_size = BATCH_SIZE,\r\n",
    "    num_train_epochs = EPOCH,\r\n",
    "    # You can also set other parameters here\r\n",
    ")\r\n",
    "\r\n",
    "# Now give all information to a trainer.\r\n",
    "trainer = Trainer(\r\n",
    "    model = model,\r\n",
    "    args = training_args,\r\n",
    "    train_dataset = train_val_dataset[\"train\"],\r\n",
    "    eval_dataset = train_val_dataset[\"test\"],\r\n",
    "    tokenizer = tokenizer,\r\n",
    "    data_collator = data_collator,\r\n",
    "    # You can also set other parameters\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "\n",
    "Training is pretty easy. Simply ask the trainer to train the model for you!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer.train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can see that Trainer saves some ckeckpoints, so you can load your model from those checkpoints if you want to fallback to a specific version."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save for future use"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "model.save_pretrained(os.path.join('model', 'finetuned'))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5bceec4a03d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'finetuned'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediction\n",
    "\n",
    "We've known how to train a model now, but how to really use it for predicting results?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load finetuned model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Same, change to TFxxxxxx if you are using tensorflow\r\n",
    "from transformers import AutoModelForSequenceClassification\r\n",
    "\r\n",
    "mymodel = AutoModelForSequenceClassification.from_pretrained(os.path.join('model', 'finetuned'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get the prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given six example sentences..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "examples = [\r\n",
    "    # A2\r\n",
    "    \"Remember to write me a letter.\",\r\n",
    "    # B2\r\n",
    "    \"Strawberries and cream - a perfect combination.\",\r\n",
    "    \"This so-called \\\"Perfect Evening\\\" was so disappointing, as well as discouraging us from coming to your Circle Theatre again.\",\r\n",
    "    # C1\r\n",
    "    \"Some may altogether give up their studies, which I think is a disastrous move.\",\r\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "...all you need to do is to transform them to embeddings, and then you can get predictions by calling your finetuned model.  \n",
    "\n",
    "Note that, since you don't have a DataCollator to pad the sentence and do the matrix transformation for you, you have to pad and transform the matrice on your own."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Transform the sentences into embeddings\r\n",
    "input = tokenizer(examples, truncation=True, padding=True, return_tensors=\"pt\") # change return_tensors if youre using tensorflow\r\n",
    "# Get the output\r\n",
    "logits = mymodel(**input).logits\r\n",
    "logits"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[  5.7654,  -7.7767,  -6.8585,  -6.9179,  -7.3156,  -7.5655],\n",
       "        [ -7.9373,  -6.8985,  -7.5784,   0.1623,  -4.7602,  -0.8337],\n",
       "        [-11.4088,  -9.5528,  -9.0424,  -8.2688,   7.2845,  -8.6719],\n",
       "        [-12.9561, -10.5447, -10.2148,   6.1383,  -6.2422,  -7.5529]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Logits aren't very readable for us. Let's use softmax activation to transform them into more probability-like numbers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Or `from tensorflow import nn` and `nn.softmax`\r\n",
    "from torch import nn\r\n",
    "\r\n",
    "predicts = nn.functional.softmax(logits, dim = -1)\r\n",
    "predicts"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[9.9999e-01, 1.3144e-06, 3.2923e-06, 3.1024e-06, 2.0843e-06, 1.6235e-06],\n",
       "        [2.2033e-04, 6.2261e-04, 3.1544e-04, 7.2558e-01, 5.2827e-03, 2.6798e-01],\n",
       "        [7.6141e-09, 4.8714e-08, 8.1153e-08, 1.7591e-07, 1.0000e+00, 1.1755e-07],\n",
       "        [5.0980e-09, 5.6841e-08, 7.9055e-08, 9.9999e-01, 4.1998e-06, 1.1324e-06]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### [ TODO ] transform logits back to labels\n",
    "\n",
    "Now you've got the output. Write a function to map it back into labels!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# [ TODO ] try to process the result\r\n",
    "label_values = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "import numpy as np\r\n",
    "def result(predicts):\r\n",
    "    result = []\r\n",
    "    predicts = predicts.detach().numpy()\r\n",
    "    label_index = np.argmax(predicts, axis=1)\r\n",
    "    for i in label_index:\r\n",
    "        result.append(label_values[i])\r\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "result(predicts)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A1\n",
      "B2\n",
      "C1\n",
      "B2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [ TODO ] Evaluation\n",
    "\n",
    "It's your turn!  \n",
    "Load the testing data and calculate your accuracy.\n",
    "\n",
    "We want you to calculate two kinds of accuracy, exact accuracy and fuzzy accuracy, which will be explained in the following section.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# [ TODO ] \r\n",
    "# load test data\r\n",
    "import os\r\n",
    "from datasets import load_dataset\r\n",
    "test_data = load_dataset('csv', data_files = os.path.join('data', 'evp.test.csv'))\r\n",
    "\r\n",
    "# get predictions\r\n",
    "# transform predictions back into labels"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-46ddc6218fff0e08\n",
      "Reusing dataset csv (C:\\Users\\Kyle\\.cache\\huggingface\\datasets\\csv\\default-46ddc6218fff0e08\\0.0.0\\bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n",
      "100%|██████████| 1/1 [00:00<00:00, 83.32it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# preprocess\r\n",
    "MODEL_NAME = 'distilbert-base-uncased'\r\n",
    "from transformers import AutoTokenizer # For tokenization\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\r\n",
    "# input = tokenizer(dataset['train']['text'], truncation=True, padding=True, return_tensors=\"pt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "#test_data: 2300 \r\n",
    "testslic = test_data['train']['text'][0:500]\r\n",
    "ground = test_data['train']['level'][0:500]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "# testslic = test_data['train']['text'][0:20]\r\n",
    "# ground = test_data['train']['level'][0:20]\r\n",
    "input = tokenizer(testslic, truncation=True, padding=True, return_tensors=\"pt\")\r\n",
    "logits = mymodel(**input).logits\r\n",
    "predicts = nn.functional.softmax(logits, dim = -1)\r\n",
    "\r\n",
    "results = []\r\n",
    "results = result(predicts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "print('results:', results)\r\n",
    "print('ground: ', ground)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "results: ['C2', 'B2', 'B2', 'B2', 'C1', 'A1', 'B1', 'B2', 'A2', 'C2', 'C2', 'C1', 'C1', 'C1', 'B1', 'B2', 'C1', 'C1', 'A2', 'C1']\n",
      "ground:  ['C2', 'B2', 'B2', 'C2', 'C1', 'A2', 'B1', 'B2', 'A2', 'C1', 'B2', 'C1', 'C1', 'C1', 'B1', 'B1', 'C1', 'C2', 'C1', 'C1']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# we still recommend you to print out some predictions to check if the outputs are resonable and if you need to adjust your model at the end of every step.\r\n",
    "\r\n",
    "# for idx, (sent, level) in enumerate(zip(test_data['text'], predict_label)):\r\n",
    "#     if idx >= 10: break\r\n",
    "#     print(f'{level}: {sent}') "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Six Level Accuracy\n",
    "\n",
    "Exact accuracy is what you've been familiar with:\n",
    "\n",
    "$\n",
    "accuracy = \\frac{\\#exactly\\:the\\:same\\:levels}{\\#total}\n",
    "$\n",
    "\n",
    "Example:\n",
    "```\n",
    "Prediction:   A1 A2 B1 B2 C1 C2\n",
    "Ground truth: A2 B1 B1 B2 B2 C2\n",
    "                    ^  ^     ^\n",
    "```\n",
    "\n",
    "The six level accuracy is $\\frac{3}{6} = 0.5$\n",
    "\n",
    "As the requirement, <u>your exact accuracy should be higher than $0.5$</u>."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "# [ TODO ] calculate accuracy\r\n",
    "count = 0\r\n",
    "same = 0\r\n",
    "for i in range(len(ground)):\r\n",
    "    if ground[i] == results[i]:\r\n",
    "        same += 1\r\n",
    "    count += 1\r\n",
    "\r\n",
    "print(same/count)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.576\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### [ TODO ] Three Level Accuracy\n",
    "\n",
    "Three Level Accuracy is used when you only want the general of right or wrong.\n",
    "\n",
    "$\n",
    "accuracy = \\frac{\\#the\\:same\\:ABC\\:levels}{\\#total}\n",
    "$\n",
    "\n",
    "Example:\n",
    "```\n",
    "Prediction:   A1 A2 B1 B2 C1 C2\n",
    "Ground truth: A2 B1 B1 B2 B2 C2\n",
    "              ^     ^  ^     ^\n",
    "```\n",
    "\n",
    "The six level accuracy is $\\frac{4}{6} = 0.667$\n",
    "\n",
    "As the requirement, <u>your exact accuracy should be higher than $0.6$</u>."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "# [ TODO ] calculate accuracy\r\n",
    "count = 0\r\n",
    "same = 0\r\n",
    "for i in range(len(ground)):\r\n",
    "    letter_g = ground[i][0]\r\n",
    "    letter_r = results[i][0]\r\n",
    "    if letter_g == letter_r:\r\n",
    "        same += 1\r\n",
    "    count += 1\r\n",
    "print(same/count)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.77\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### [ TODO ] Fuzzy accuracy\n",
    "\n",
    "However, the level of a sentence is relatively subjective. Generally speaking, $\\pm1$ errors are allowed in the real evaluation in linguistic area.  \n",
    "\n",
    "For example, if the label is actually 'B1', but the model predicts 'B2', we still consider the prediction good enough, and this also applys when the model predicts 'A2'.\n",
    "\n",
    "Hence, the fuzzy accuracy is\n",
    "\n",
    "$\n",
    "accuracy = \\frac{\\#good\\:enough\\:answers}{\\#total}\n",
    "$\n",
    "\n",
    "Example:\n",
    "```\n",
    "Prediction:   0 1 2 3 4 5\n",
    "Ground truth: 0 1 1 3 3 3\n",
    "              ^ ^ ^ ^ ^\n",
    "```\n",
    "\n",
    "The fuzzy accuracy is $\\frac{5}{6} = 0.833$\n",
    "\n",
    "As the requirement, <u>your accuracy should be higher than $0.8$</u>."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "# [ TODO ] calculate accuracy\r\n",
    "results_values = [label_values.index(result) for result in results ]\r\n",
    "ground_values = [label_values.index(label) for label in ground]\r\n",
    "\r\n",
    "good = 0\r\n",
    "count = 0\r\n",
    "for i in range(len(ground_values)):\r\n",
    "    if abs(ground_values[i] - results_values[i]) <= 1:\r\n",
    "        good += 1\r\n",
    "    count += 1\r\n",
    "print(good/count)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.886\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TA's note\n",
    "\n",
    "Congratuation! You've finished the assignment this week.  \n",
    "Don't forget to <b>[make an appoiment with TA](https://docs.google.com/spreadsheets/d/1QGeYl5dsD9sFO9SYg4DIKk-xr-yGjRDOOLKZqCLDv2E/edit#gid=134737606) to demo/explain your implementation <u>before <font color=\"red\">12/23 15:30</font></u></b> .  \n",
    "Also make sure you submit your `{student_id}.ipynb` to [eeclass](https://eeclass.nthu.edu.tw/course/homework/6053).\n",
    "\n",
    "This is the last assignment of this class. A TA will still be at the online classroom and answer your question during the class time in the following weeks, and you can have make-up demos at that time.  \n",
    "Prof. Chang's office hours are in Tues. to Thurs. evenings. You can come to Delta 712 to consult him at that time, but make sure you follow the appointment rules written on the bulletin or [the appointment sheet](https://docs.google.com/spreadsheets/d/1QGeYl5dsD9sFO9SYg4DIKk-xr-yGjRDOOLKZqCLDv2E/edit?usp=sharing).\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Appendix \n",
    "\n",
    "<a name=\"Appendix-1-Install-CUDA\"></a>\n",
    "\n",
    "### Appendix 1 - Install CUDA\n",
    "\n",
    "1. Check your GPU vs. CUDA compatibility:\n",
    "   - [NVIDIA -> Your GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) -> GeForce and TITAN Products\n",
    "2. Check library vs. CUDA compatibility: \n",
    "   - Pytorch: [Previous PyTorch Versions](https://pytorch.org/get-started/previous-versions/)\n",
    "   - Tensorflow: [Linux/MacOX](https://www.tensorflow.org/install/source#tested_build_configurations) or [Windows](https://www.tensorflow.org/install/source_windows#tested_build_configurations)\n",
    "3. Note the highest CUDA version that fits your system.\n",
    "\n",
    "#### >> for conda/mamba users\n",
    "\n",
    "You can directly install CUDA library with the selected CUDA version.\n",
    "1. Get [the driver for NVIDIA GPU](https://www.nvidia.com/download/index.aspx)\n",
    "2. `conda/mamba install -c conda-forge cudatoolkit=${VERSION}`\n",
    "\n",
    "#### >> for non-conda users\n",
    "\n",
    "1. Get [the driver for NVIDIA GPU](https://www.nvidia.com/download/index.aspx)\n",
    "2. Download and install [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive)\n",
    "3. Download and install [cuDNN Library](https://developer.nvidia.com/rdp/cudnn-archive)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name=\"Appendix-2-TAs-Environmental-setup\"></a>\n",
    "\n",
    "### Appendix 2 - TA's Environmental Setup\n",
    "\n",
    "The following is my setup for this notebook. You can refer to it if you encounter some environmental issues.  \n",
    "\n",
    "System: Ubuntu 18.04.6, x64, With GPU support. All packages are installed in new conda environment with channels default to conda-forge.\n",
    "\n",
    "1. Python 3.8.12\n",
    "2. numpy=1.21.2\n",
    "3. cudatoolkit=11.1.74\n",
    "4. pytorch=1.8.2\n",
    "5. datasets=1.16.1\n",
    "6. transformers=4.12.5\n",
    "7. scikit-learn=1.0.1\n",
    "\n",
    "Notes:\n",
    "\n",
    " - conda create -n week14 python=3.8 & conda activate week14\n",
    " - conda config --add channels conda-forge\n",
    " - conda config --set channel_priority strict\n",
    " - conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch-lts -c nvidia\n",
    " - conda install transformers\n",
    " - conda install datasets scikit-learn\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Appendix 3 - Further Readings\n",
    "\n",
    "1. [Huggingface Official Tutorials](https://github.com/huggingface/notebooks/tree/master/examples)\n",
    "2. How to use Bert with other downstream tasks: [How to use BERT from the Hugging Face transformer library](https://towardsdatascience.com/how-to-use-bert-from-the-hugging-face-transformer-library-d373a22b0209): \n",
    "3. Training with pytorch backend: [transformers-tutorials](https://github.com/abhimishra91/transformers-tutorials)\n",
    "4. A more complicated example that include manual data/training processing with Pytorch: [Transformers for Multi-Label Classification made simple](https://towardsdatascience.com/transformers-for-multilabel-classification-71a1a0daf5e1)\n",
    "5. [Text Classification with tensorflow](https://github.com/huggingface/notebooks/blob/master/examples/text_classification-tf.ipynb): tensorflow example"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "71b71b123f4f9f0ec8a028979ae4fa380b64ac835e25e067feacb038fad71296"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('pytorch': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}